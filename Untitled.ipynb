{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import nltk, re, glob\n",
    "import numpy as np\n",
    "from nltk.tokenize import casual_tokenize\n",
    "from nltk.tokenize.casual import _replace_html_entities\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk import word_tokenize, pos_tag\n",
    "from sklearn.datasets import load_files\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "#from sklearn.tree import DecisionTreeRegression\n",
    "from sklearn import tree\n",
    "\n",
    "from sklearn import svm\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "import itertools\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.feature_selection import RFECV\n",
    "from sklearn.feature_selection import SelectFromModel\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.feature_selection import SelectKBest\n",
    "from sklearn.feature_selection import chi2\n",
    "from sklearn.feature_selection import VarianceThreshold\n",
    "from nltk.stem.lancaster import LancasterStemmer\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.naive_bayes import BernoulliNB\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.feature_extraction import text \n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.decomposition import TruncatedSVD\n",
    "from sklearn.svm import LinearSVC\n",
    "\n",
    "lancaster_stemmer = LancasterStemmer()\n",
    "\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "EMOTICONS = r\"\"\"\n",
    "    (?:\n",
    "      [<>]?\n",
    "      [:;=8]                     # eyes\n",
    "      [\\-o\\*\\']?                 # optional nose\n",
    "      [\\)\\]\\(\\[dDpP/\\:\\}\\{@\\|\\\\] # mouth\n",
    "      |\n",
    "      [\\)\\]\\(\\[dDpP/\\:\\}\\{@\\|\\\\] # mouth\n",
    "      [\\-o\\*\\']?                 # optional nose\n",
    "      [:;=8]                     # eyes\n",
    "      [<>]?\n",
    "      |\n",
    "      <3                         # heart\n",
    "    )\"\"\"\n",
    "\n",
    "# URL pattern due to John Gruber, modified by Tom Winzig. See\n",
    "# https://gist.github.com/winzig/8894715\n",
    "\n",
    "URLS = r\"(?i)\\b((?:https?:(?:/{1,3}|[a-z0-9%])|[a-z0-9.\\-]+[.](?:[a-z]{2,13})(?:[^\\s()<>{}\\[\\]]+|\\([^\\s()]*?\\([^\\s()]+\\)[^\\s()]*?\\)|\\([^\\s]+?\\))+(?:\\([^\\s()]*?\\([^\\s()]+\\)[^\\s()]*?\\)|\\([^\\s]+?\\)|[^\\s`!()\\[\\]{};:'\\\".,<>?¬´¬ª‚Äú‚Äù‚Äò‚Äô])|(?:(?<!@)[a-z0-9]+(?:[.\\-][a-z0-9]+)*[.](?:[a-z]{2,13})\\b/?(?!@))))\"\n",
    "\n",
    "\n",
    "def clean_first(doc):\n",
    "    # make everything lower case \n",
    "    words = doc.lower()   \n",
    "    # Replace  URLs but some URLs becomes \"URLURL\"\n",
    "    words = re.sub(URLS,\"URL\", words)\n",
    "    # Replace Numbers \n",
    "    # (But we get stuff like 'NUM-NUM' 'NUM.NUMNUM' 'NUMNUM' 'NUMNUM-NUMNUM' 'NUMNUMNUM'\n",
    "    # 'NUMNUMNUMNUM' 'NUMNUMk' 'NUMnd' 'NUMwhy')\n",
    "    #cleaned = re.sub('\\d+', \"NUM\", words)\n",
    "    cleaned = re.sub(\"[^a-zA-Z\\s\\W]+\", \"\", words) #replace NUM with nothing\n",
    "   # cleaned = re.sub(r\"([\\w\\d]+\\.)([\\w\\d]+)\", r\"\\1 \\2\", words)\n",
    "   # print(cleaned)\n",
    "    return(cleaned)\n",
    "   \n",
    "def lemmatize(doc):   \n",
    "    lemma_list = []\n",
    "    wnl = WordNetLemmatizer()\n",
    "    # cleans the document\n",
    "    cleaned_doc = clean_first(doc)\n",
    "    # split into sentences\n",
    "    sent_text = nltk.sent_tokenize(cleaned_doc)\n",
    "    for line in sent_text:\n",
    "    # lemmatize with casual_tokenizer from http://www.nltk.org/_modules/nltk/tokenize/casual.html#TweetTokenizer (we can use something else)\n",
    "        lemma_list1 = [wnl.lemmatize(i,j[0].lower()) if j[0].lower() in ['a','n','v'] else wnl.lemmatize(i) for i,j in pos_tag(casual_tokenize(line))]\n",
    "        lemma_list = lemma_list + lemma_list1\n",
    "    return(lemma_list)\n",
    "\n",
    "def stemmer(doc):\n",
    "    stem_list= []\n",
    "    l_stemmer = LancasterStemmer()\n",
    "    cleaned_doc = clean_first(doc)\n",
    "    sent_text = nltk.sent_tokenize(cleaned_doc)\n",
    "    for line in sent_text:\n",
    "        for word in nltk.word_tokenize(line):\n",
    "            stem_list.append(l_stemmer.stem(word))\n",
    "    return(stem_list)\n",
    "\n",
    "def lem_stemmer(doc):\n",
    "    stem_list=[]\n",
    "    lemma = lemmatize(doc)\n",
    "    l_stemmer = LancasterStemmer()\n",
    "    for word in lemma:\n",
    "        stem_list.append(l_stemmer.stem(word))\n",
    "    return(stem_list)\n",
    "    \n",
    "def make_combs():\n",
    "    combs = []\n",
    "    poslist=[ 'j', 'n', 'r','v']\n",
    "    #'CC', 'DT', 'EX',  'UH' 'TO' , 'w'  'i',  'm',\n",
    "    for L in range(0, len(poslist)+1): \n",
    "        for subset in itertools.combinations(poslist, L): # get all the possible combination of POSs\n",
    "            if subset != ():  \n",
    "                if len(subset) < 5:\n",
    "                    combs.append(subset) # append in the list \"comb\"\n",
    "    return combs\n",
    "    #print(combs[1])\n",
    "    \n",
    "def get_selected_lemma(doc,comb):\n",
    "   # combs = make_combs()\n",
    "    lemma_list = []\n",
    "    wnl = WordNetLemmatizer()        \n",
    "    # cleans the document\n",
    "    cleaned_doc = clean_first(doc)\n",
    "    # split into sentences\n",
    "    sent_text = nltk.sent_tokenize(cleaned_doc)\n",
    "    for line in sent_text:\n",
    "    # lemmatize with casual_tokenizer from http://www.nltk.org/_modules/nltk/tokenize/casual.html#TweetTokenizer (we can use something else)\n",
    "       # lemma_list1 = [wnl.lemmatize(i,j[0].lower()) if j[0].lower() in ['a','n','v'] else wnl.lemmatize(i) for i,j in pos_tag(casual_tokenize(line))]\n",
    "        for j in pos_tag(casual_tokenize(line)):\n",
    "           \n",
    "            if str(j[1][0].lower()) in comb:\n",
    "                if j[1][0].lower() in ['j']: #,'n','v']:\n",
    "                    lemma = wnl.lemmatize(j[0],'a')\n",
    "                elif j[1][0].lower() in ['n','v']:\n",
    "                    lemma = wnl.lemmatize(j[0],j[1][0].lower())\n",
    "                else:\n",
    "                    lemma = wnl.lemmatize(j[0])\n",
    "                lemma_list.append(lemma)\n",
    "                \n",
    "    return(lemma_list)\n",
    "\n",
    "\n",
    "# lemmatize and make n-grams\n",
    "def make_ngrams(text,n):\n",
    "    ngram_list = []\n",
    "    text = re.sub(r\"([\\w\\d]+[\\.!?]+)([\\w\\d]+)\", r\"\\1 \\2\", text)\n",
    "    sent_text = nltk.sent_tokenize(text) \n",
    "    #print(sent_text)\n",
    "    for sentence in sent_text:\n",
    "        lemma_list = lemmatize(sentence) \n",
    "        #print(lemma_list)\n",
    "        for i in range(len(lemma_list) - (n - 1)):\n",
    "            ngram = lemma_list[i:i+n]\n",
    "            stringngram = \" \".join(ngram) #turn the list into string\n",
    "            ngram_list.append(stringngram) #append the n-gram of words into a list\n",
    "    return ngram_list\n",
    "\n",
    "def make_ChNgrams(text, n): \n",
    "    ngram_list=[]\n",
    "    text = re.sub(r\"([\\w\\d]+[\\.!?]+)([\\w\\d]+)\", r\"\\1 \\2\", text)\n",
    "    sent_text = nltk.sent_tokenize(text) \n",
    "    for sentence in sent_text:\n",
    "        #sentence = sentence.strip([\\n\\t])\n",
    "        new_sent = re.sub(\"/\\s+/S\",\"\",sentence) # remove spaces\n",
    "        for i in range(len(new_sent) - (n-1)):\n",
    "            ngram=new_sent[i:i+n]\n",
    "            ngram_list.append(ngram)\n",
    "    return ngram_list \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feature: tfidf "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "samples per class: [759 840]\n",
      "Data: [255 278]\n",
      "X_train.shape:\n",
      "(1599, 29682)\n",
      "features with highest tfidf:\n",
      "['species' 'tru' 'kara' 'sx' 'theydidthefuckyou' 'yoga' '\\u200d' 'schwartz'\n",
      " 'alexa' '‚ô•' 'hawaii' 'nga' 'sht' 'grassroots' 'taskbar' 'pokeball'\n",
      " 'piercer' 'toefl' 'photojournalist' \"juice's\" 'vector' 'puzzle' 'xda'\n",
      " 'repost' 'wholesomeness' 'violent' 'ü§ó' 'trader' 'cheapo' 'gd' 'chester'\n",
      " \"o'reilly\" 'cam' 'naomi' 'trusted' 'darvish' 'otp' 'pounder' 'phish'\n",
      " 'snort' 'fv' 'opium' '<' 'lsu' 'brothel' 'clause' 'porygon' 'kadabra'\n",
      " 'cleft' 'dbd' 'deoderant' 'epileptic' 'jinder' 'stamford' 'sperm' 'porky'\n",
      " 'ahaha' 'Ô∏è' 'mastery' 'nitro' 'coke' 'vox' 'neighbourhood' 'skaven'\n",
      " 'jansen' 'dota' 'goku' '(:' 'nitros' 'fatale' 'deposit' 'trata' 'batch'\n",
      " '‚úä' 'jailbreak' 'orthos' 'tortoise' 'illenium' 'bu' 'messi' 'petrol' 'üëà'\n",
      " 'hatch' 'fcking' 'üôà' 'warranty' 'sapphire' 'tarantula' 'router' 'qc'\n",
      " 'usersimulator' 'wenger' 'lap' 'feng' 'üèΩ' 'beany' 'skype' 'verry'\n",
      " 'registration' 'infp' 'dxm' 'mio' 'üëç' 'gluten' 'sosa' 'üíÄ' 'turquoise'\n",
      " 'meera' 'laborer' 'database' 'fender' 'retainer' 'pax' 'policeman'\n",
      " 'generous' 'mei' 'vegan' 'mc' 'rhinoplasty' 'cpu' 'honorary' 'pie' 'parry'\n",
      " 'karambit' 'masturbating' 'hamilton' 'shrimp' 'dogecoin' 'grey' 'ladybug'\n",
      " 'birb' 'teddy' 'giannis' 'coil' 'coc' 'sojiro' '<--' 'hz' 'tar' 'wiggle'\n",
      " 'plymouth' 'dentist' 'lucina' 'lvl' 'ucla' 'suarez' 'hu' 'radiance'\n",
      " 'calculus' 'farcry' 'terrence' 'rax' '‚Ä¢' 'goldberg' 'redeem' 'incursion'\n",
      " 'falmer' 'fye' 'sair' 'pua' 'orleans' 'defenseman' 'psycho' 'warframe'\n",
      " 'confidant' 'tournies' 'tele' 'zeppelin' 'isaac' ':]' 'integer' 'lonzo'\n",
      " 'mmr' 'Õü' 'lowercase' 'folder' 'cyborg' 'junipero' 'sigma' 'kesh' 'lck'\n",
      " 'youll' 'vidya' 'steamid' 'zoro' 'jayce' 'crust' '|' 'sk' 'escort'\n",
      " 'denmark' 'ticking' 'battlefield' 'yorkie' '¬¥' '`' 'memorize' 'Õû' 'fare'\n",
      " 'trucker' 'doppler' 'pho' 'beau' 'campground' ';-)' 'fifa' 'remix' 'ortho'\n",
      " 'sabbath' 'dysphoria' 'sydney' 'culinary' 'fut' 'guidance' 'jailor'\n",
      " 'rehearsal' 'hadith' 'Ã∏' 'scp' 'armin' 'totoro' 'jake' 'üëÅ' 'vyvanse'\n",
      " 'kneeler' 'cersei' 'rune' 'ahhh' 'ign' '‚ÄΩ' 'deco' 'freddy' 'wherein'\n",
      " 'bayonet' 'diane' 'nofap' 'chic' 'phlebotomist' 'deku' 'hum' 'neverwinter'\n",
      " \"i'v\" 'tyrone' 'mailman' 'afrikaans' 'woof' 'kenyan' 'strigoi' 'vaporwave'\n",
      " 'utopia' 'üåà' '‚Äî' 'bod' 'pk' 'onenote' 'counsellor' 'dis' 'indentured'\n",
      " 'yee' 'ko' 'seizure' 'ore' 'parrot' '^' 'xdd' 'mgtow' 'lin' 'coz' 'fn'\n",
      " 'wts' 'nib' 'reverb' 'greg' 'nmom' 'mdma' '->' 'cuphead' 'voucher' 'sd'\n",
      " 'dmt' 'eren' 'sikh' 'joi' 'bdp' 'globalism' 'citalopram' 'üíõ' '.. ..'\n",
      " 'incredibles' 'fooking' 'üíØ' 'cameroon' 'üíö' 'pesto' '‚ô°' 'libtards' 'doge'\n",
      " '::' 'chur' 'üá≥']\n",
      "features with lowest tfidf:\n",
      "['situationally' 'sophisticate' 'unspecified' 'latent' 'disorientate'\n",
      " 'computation' 'low-tech' 'stacking' 'townsend' 'somos' 'anteriormente'\n",
      " 'sustenta' 'podemos' 'seguir' 'considerado' 'pblica' 'doppelganger'\n",
      " 'extraneous' 'innovate' 'hinky' 'latrine' 'enorme' 'permite' 'viso'\n",
      " 'insignificante' 'viver' 'vivendo' 'sinceramente' 'possvel' 'temos'\n",
      " 'mente' 'segundo' 'profissional' 'fao' 'presente' 'nenhuma' 'fato'\n",
      " 'entendi' 'buscam' 'tamanho' 'gasto' 'suficiente' 'feliz' 'coworking'\n",
      " 'ambiente' 'diferena' 'hoje' 'nessa' 'triste' 'devemos' 'vestibular'\n",
      " 'sabe' 'falta' 'algumas' 'haga' 'horas' 'gape' 'gmo' 'epsom' 'legume'\n",
      " 'emanate' 'confines' 'diferente' 'perder' 'flowchart' 'identifiable'\n",
      " 'to-the-point' 'haciendo' 'bsico' 'siquiera' 'rentar' 'sala' 'estn' 'cmo'\n",
      " 'estaban' 'podra' 'derecho' 'podran' 'centro' 'acciones' 'casi' 'glazing'\n",
      " 'unlisted' 'meathead' 'pylorus' 'prudent' 'fuder' 'fazendo'\n",
      " 'aposentadoria' 'negcio' 'soluo' 'gostam' 'efeitos' 'industria' 'corpo'\n",
      " 'vivo' 'afeta' 'perto' 'necrosis' 'predispose' 'freer' 'safeguard'\n",
      " 'broadly' 'overcoat' 'hermetic' 'egoism' 'squarely' \"hemingway's\" 'ernest'\n",
      " 'errant' 'agrarian' 'theologian' 'precocious' \"branch's\" 'provable'\n",
      " 'acclimate' 'documented' 'complicating' 'majoirty' 'shone' 'avoids'\n",
      " 'ascribe' 'transcendent' 'decorated' 'spewing' 'unfathomable'\n",
      " 'deactivation' 'memorialize' 'shrapnel' 'decentralize' 'candlestick'\n",
      " 'regulatory' 'voxels' \"joyce's\" 'discovers' 'idealized' 'firmer' 'vilify'\n",
      " 'discrediting' 'recalibrate' 'discernment' 'accolade' 'cryptomarket'\n",
      " 'volatility' 'chelate' 'reveals' 'ionic' 'bipedal' 'permutation'\n",
      " 'neglected' 'extraverted' 'intimidated' 'facilitate' 'sicily' 'masonry'\n",
      " 'geocaching' 'creepily' 'validating' 'justnomil' 'acclaim' 'sexualization'\n",
      " 'unitarian' 'arpaio' 'bumfuck' 'slacktivism' 'deride' 'feral' 'treatise'\n",
      " 'gastos' 'peso' 'correspondence' 'cede' 'unsanitary' 'occurring'\n",
      " 'bestiary' 'non-political' 'trusting' 'hooah' 'deployed' 'inexplicably'\n",
      " 'pritchard' 'cacao' 'guzzle' 'middle-eastern' 'espouse' 'well-read'\n",
      " 'stagnation' 'perceptible' 'homelife' 're-evaluation' 'toothed' 'misc'\n",
      " 'forhire' 'defiant' 'renege' 'homemaking' 'sahm' 'colloquially' 'focusing'\n",
      " 'lentils' 'telecommunication' 'scattered' 'prion' 'prevalence'\n",
      " 'geological' 'westernize' 'bolivia' 'decry' 'reciprocal' 'protracted'\n",
      " 'pre-cum' 'non-existence' 'untruth' \"r's\" 'inline' 'parte' 'lado' 'hora'\n",
      " 'unbothered' 'gobshite' 'cistern' 'dogpiled' 'weighted' 'meticulously'\n",
      " 'sleeep' 'poser' 'evocation' 'employable' 'aspie' 'hiit' 'cima'\n",
      " 'undertake' 'focussed' 'well-made' 'exemplifies' 'untimely' 'vaguest'\n",
      " 'fanatical' 'apu' 'retiree' 'gonad' 'terabyte' 'taiga' 'self-criticism'\n",
      " 'reaching' 'hyaluronic' 'investigative' 'heirloom' 'disallow'\n",
      " 'uncommitted' 'fuckup' 'unjustifiable' 'discordant' 'occassions'\n",
      " 'vindicate' 'urgency' 'alarmist' 'angelic' 'wrathful' 'platonically'\n",
      " 'hand-waved' 'piety' 'multi-billion' 'diligence' 'suprisingly'\n",
      " 'simplified' 'crossbones' 'cns' 'trabalho' 'ouvi' 'plano' 'outros'\n",
      " 'self-fulfilling' 'sido' 'lugares' 'unequivocal' 'expressly' 'bemoan'\n",
      " 'tickling' 'caged' 'eco-friendly' 'anti-gmo' 'invalidation' 'carnivore'\n",
      " 'repairman' 'absurdly' 'reservist' 'off-color' 'fraught' 'rulebook'\n",
      " 'torturer' 'harassed' 'retin-a' 'bons' 'brainstorm' 'pierced' 'nomadic'\n",
      " 'quart' 'first-year' 'humanist']\n",
      "\n",
      "no feature selection\n",
      "\n",
      "NB Test 0.555\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "        Dep       0.88      0.08      0.15       255\n",
      "         ND       0.54      0.99      0.70       278\n",
      "\n",
      "avg / total       0.70      0.56      0.44       533\n",
      "\n",
      "SVM Test score: 0.522\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "        Dep       0.00      0.00      0.00       255\n",
      "         ND       0.52      1.00      0.69       278\n",
      "\n",
      "avg / total       0.27      0.52      0.36       533\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/mh/anaconda3/lib/python3.6/site-packages/sklearn/metrics/classification.py:1113: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LR Test score: 0.704\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "        Dep       0.77      0.55      0.64       255\n",
      "         ND       0.67      0.85      0.75       278\n",
      "\n",
      "avg / total       0.72      0.70      0.70       533\n",
      "\n",
      "X_train_pca.shape: (1599, 100)\n",
      "GaussianNB Test 0.540\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "        Dep       0.77      0.55      0.64       255\n",
      "         ND       0.67      0.85      0.75       278\n",
      "\n",
      "avg / total       0.72      0.70      0.70       533\n",
      "\n",
      "SVM Test score: 0.522\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "        Dep       0.00      0.00      0.00       255\n",
      "         ND       0.52      1.00      0.69       278\n",
      "\n",
      "avg / total       0.27      0.52      0.36       533\n",
      "\n",
      "LR Test score: 0.698\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "        Dep       0.75      0.55      0.64       255\n",
      "         ND       0.67      0.83      0.74       278\n",
      "\n",
      "avg / total       0.71      0.70      0.69       533\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'\\nprint(\\'feature selection\\n\\')\\nlr = LogisticRegression(C=1, penalty=\\'l1\\').fit(X_train, y_train)\\nmodel = SelectFromModel(lr, prefit=True)\\nX_new = model.transform(X_train)\\nprint(X_new.shape)\\n\\n#print(X_new.get_feature_names())\\nX_test_1= model.transform(X_test)\\n#print(\"X_test:\\n{}\".format(repr(X_test_1)))\\n\\npredscore = mnNB.fit(X_new,y_train).score(X_test_1,y_test)\\nprint(\\'NB Test {:.3f}\\'.format(predscore))\\n\\npredscore = svm.SVC().fit(X_new,y_train).score(X_test_1,y_test)\\nprint(\"SVM Test score: {:.3f}\".format(predscore))\\n\\npredscore = LogisticRegression().fit(X_new, y_train).score(X_test_1,y_test)\\nprint(\"LR Test score: {:.3f}\".format(predscore))\\n'"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stopset = set(nltk.corpus.stopwords.words('english'))\n",
    "stopset.add(\"\\'\")\n",
    "stopset.add(\"\\\"\")\n",
    "\n",
    "reddit_train = load_files(\"/media/mh/EF2A-B9DB/Reddit_Data/Train/\")\n",
    "X, y = reddit_train.data, reddit_train.target\n",
    "# Check the data\n",
    "\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.25, random_state=42)\n",
    "\n",
    "print(\"samples per class: {}\".format(np.bincount(y_train)))\n",
    "print(\"Data: {}\".format(np.bincount(y_test)))\n",
    "#print(X_train.shape)\n",
    "\n",
    "\n",
    "\n",
    "#lemma_vect = CountVectorizer(tokenizer=lambda doc: get_selected_lemma(doc,comb), stop_words =stopset, min_df=2)\n",
    "vect = TfidfVectorizer(analyzer='word', tokenizer=lemmatize, sublinear_tf=True, min_df=2,stop_words=stopset)\n",
    "X_train = vect.fit_transform(X_train)\n",
    "print('X_train.shape:\\n{}'.format(X_train.shape))\n",
    "\n",
    "\n",
    "# To check features (the first 200)\n",
    "#    feature_names_lemma = np.array(lemma_vect.get_feature_names())\n",
    "#    print(feature_names_lemma[2000:2500])\n",
    "\n",
    "max_value = X_train.max(axis=0).toarray().ravel()\n",
    "sorted_by_tfidf = max_value.argsort()\n",
    "feature_names=np.array(vect.get_feature_names())\n",
    "print(\"features with highest tfidf:\\n{}\".format(feature_names[sorted_by_tfidf[-300:]]))\n",
    "print(\"features with lowest tfidf:\\n{}\".format(feature_names[sorted_by_tfidf[:300]]))\n",
    "\n",
    "\n",
    "\n",
    "mnNB = MultinomialNB()\n",
    "\n",
    "\n",
    "print(\"\\nno feature selection\\n\")\n",
    "\n",
    "X_test = vect.transform(X_test) \n",
    "#        print('X.shape:\\n{}'.format(X_train.shape))\n",
    "\n",
    "predict = mnNB.fit(X_train,y_train).predict(X_test)\n",
    "\n",
    "\n",
    "predscore = mnNB.fit(X_train,y_train).score(X_test,y_test)\n",
    "print('NB Test {:.3f}'.format(predscore))\n",
    "\n",
    "print(classification_report(y_test, predict, target_names=['Dep','ND']))\n",
    "#  print(confusion_matrix(y_test, y_pred, labels=range(n_classes)))\n",
    "\n",
    "predict = svm.SVC().fit(X_train,y_train).predict(X_test)\n",
    "predscore = svm.SVC().fit(X_train,y_train).score(X_test,y_test)\n",
    "print(\"SVM Test score: {:.3f}\".format(predscore))\n",
    "print(classification_report(y_test, predict, target_names=['Dep','ND']))\n",
    "\n",
    "predict = LogisticRegression().fit(X_train,y_train).predict(X_test)\n",
    "predscore = LogisticRegression().fit(X_train,y_train).score(X_test,y_test)\n",
    "print(\"LR Test score: {:.3f}\".format(predscore))\n",
    "print(classification_report(y_test, predict, target_names=['Dep','ND']))\n",
    "\n",
    "\n",
    "pca = TruncatedSVD(n_components=100, random_state=0).fit(X_train)\n",
    "#logistic = LogisticRegression()\n",
    "# pipe = Pipeline(steps=[('pca', pca), ('logistic', logistic)])\n",
    "X_train_pca = pca.transform(X_train)\n",
    "X_test_pca = pca.transform(X_test)\n",
    "print(\"X_train_pca.shape: {}\".format(X_train_pca.shape))\n",
    "\n",
    "\n",
    "\n",
    "predscore = GaussianNB().fit(X_train_pca,y_train).score(X_test_pca,y_test)\n",
    "print('GaussianNB Test {:.3f}'.format(predscore))\n",
    "\n",
    "print(classification_report(y_test, predict, target_names=['Dep','ND']))\n",
    "# print(confusion_matrix(y_test, y_pred, labels=range(n_classes)))\n",
    "\n",
    "predict = svm.SVC().fit(X_train_pca,y_train).predict(X_test_pca)\n",
    "predscore = svm.SVC().fit(X_train_pca,y_train).score(X_test_pca,y_test)\n",
    "print(\"SVM Test score: {:.3f}\".format(predscore))\n",
    "print(classification_report(y_test, predict, target_names=['Dep','ND']))\n",
    "\n",
    "predict = LogisticRegression().fit(X_train_pca,y_train).predict(X_test_pca)\n",
    "predscore = LogisticRegression().fit(X_train_pca,y_train).score(X_test_pca,y_test)\n",
    "print(\"LR Test score: {:.3f}\".format(predscore))\n",
    "print(classification_report(y_test, predict, target_names=['Dep','ND']))\n",
    "\n",
    "\"\"\"\n",
    "print('feature selection\\n')\n",
    "lr = LogisticRegression(C=1, penalty='l1').fit(X_train, y_train)\n",
    "model = SelectFromModel(lr, prefit=True)\n",
    "X_new = model.transform(X_train)\n",
    "print(X_new.shape)\n",
    "\n",
    "#print(X_new.get_feature_names())\n",
    "X_test_1= model.transform(X_test)\n",
    "#print(\"X_test:\\n{}\".format(repr(X_test_1)))\n",
    "\n",
    "predscore = mnNB.fit(X_new,y_train).score(X_test_1,y_test)\n",
    "print('NB Test {:.3f}'.format(predscore))\n",
    "\n",
    "predscore = svm.SVC().fit(X_new,y_train).score(X_test_1,y_test)\n",
    "print(\"SVM Test score: {:.3f}\".format(predscore))\n",
    "\n",
    "predscore = LogisticRegression().fit(X_new, y_train).score(X_test_1,y_test)\n",
    "print(\"LR Test score: {:.3f}\".format(predscore))\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_train_pca.shape: (1599, 900)\n",
      "GaussianNB Test 0.480\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "        Dep       0.77      0.57      0.65       255\n",
      "         ND       0.68      0.85      0.75       278\n",
      "\n",
      "avg / total       0.72      0.71      0.71       533\n",
      "\n",
      "SVM Test score: 0.522\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "        Dep       0.00      0.00      0.00       255\n",
      "         ND       0.52      1.00      0.69       278\n",
      "\n",
      "avg / total       0.27      0.52      0.36       533\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/mh/anaconda3/lib/python3.6/site-packages/sklearn/metrics/classification.py:1113: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LR Test score: 0.700\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "        Dep       0.76      0.55      0.64       255\n",
      "         ND       0.67      0.84      0.74       278\n",
      "\n",
      "avg / total       0.71      0.70      0.69       533\n",
      "\n"
     ]
    }
   ],
   "source": [
    "pca = TruncatedSVD(n_components=900, random_state=42).fit(X_train)\n",
    "#logistic = LogisticRegression()\n",
    "# pipe = Pipeline(steps=[('pca', pca), ('logistic', logistic)])\n",
    "X_train_pca = pca.transform(X_train)\n",
    "X_test_pca = pca.transform(X_test)\n",
    "print(\"X_train_pca.shape: {}\".format(X_train_pca.shape))\n",
    "\n",
    "\n",
    "\n",
    "predscore = GaussianNB().fit(X_train_pca,y_train).score(X_test_pca,y_test)\n",
    "print('GaussianNB Test {:.3f}'.format(predscore))\n",
    "\n",
    "print(classification_report(y_test, predict, target_names=['Dep','ND']))\n",
    "# print(confusion_matrix(y_test, y_pred, labels=range(n_classes)))\n",
    "\n",
    "predict = svm.SVC().fit(X_train_pca,y_train).predict(X_test_pca)\n",
    "predscore = svm.SVC().fit(X_train_pca,y_train).score(X_test_pca,y_test)\n",
    "print(\"SVM Test score: {:.3f}\".format(predscore))\n",
    "print(classification_report(y_test, predict, target_names=['Dep','ND']))\n",
    "\n",
    "predict = LogisticRegression().fit(X_train_pca,y_train).predict(X_test_pca)\n",
    "predscore = LogisticRegression().fit(X_train_pca,y_train).score(X_test_pca,y_test)\n",
    "print(\"LR Test score: {:.3f}\".format(predscore))\n",
    "print(classification_report(y_test, predict, target_names=['Dep','ND']))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "Selected Lemmas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "samples per class: [759 840]\n",
      "Data: [255 278]\n",
      "j\n",
      "X_train.shape:\n",
      "(1599, 9161)\n",
      "\n",
      "no feature selection\n",
      "\n",
      "NB Test 0.642\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "        Dep       0.65      0.56      0.60       255\n",
      "         ND       0.64      0.72      0.68       278\n",
      "\n",
      "avg / total       0.64      0.64      0.64       533\n",
      "\n",
      "SVM Test score: 0.570\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "        Dep       0.70      0.18      0.28       255\n",
      "         ND       0.55      0.93      0.69       278\n",
      "\n",
      "avg / total       0.62      0.57      0.50       533\n",
      "\n",
      "LR Test score: 0.608\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "        Dep       0.59      0.58      0.58       255\n",
      "         ND       0.62      0.64      0.63       278\n",
      "\n",
      "avg / total       0.61      0.61      0.61       533\n",
      "\n",
      "feature selection\n",
      "\n",
      "(1599, 781)\n",
      "NB Test 0.612\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "        Dep       0.65      0.56      0.60       255\n",
      "         ND       0.64      0.72      0.68       278\n",
      "\n",
      "avg / total       0.64      0.64      0.64       533\n",
      "\n",
      "SVM Test score: 0.619\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "X.shape[1] = 9161 should be equal to 781, the number of features at training time",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-51-a1e974973798>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     69\u001b[0m         \u001b[0mpredscore\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msvm\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mSVC\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_new\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0my_train\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mscore\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_test_1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0my_test\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     70\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"SVM Test score: {:.3f}\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpredscore\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 71\u001b[0;31m         \u001b[0mpredict\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msvm\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mSVC\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_new\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0my_train\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_test\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     72\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mclassification_report\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpredict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget_names\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'Dep'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'ND'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     73\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/mh/anaconda3/lib/python3.6/site-packages/sklearn/svm/base.py\u001b[0m in \u001b[0;36mpredict\u001b[0;34m(self, X)\u001b[0m\n\u001b[1;32m    571\u001b[0m             \u001b[0mClass\u001b[0m \u001b[0mlabels\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0msamples\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    572\u001b[0m         \"\"\"\n\u001b[0;32m--> 573\u001b[0;31m         \u001b[0my\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mBaseSVC\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    574\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclasses_\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtake\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0masarray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mintp\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    575\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/mh/anaconda3/lib/python3.6/site-packages/sklearn/svm/base.py\u001b[0m in \u001b[0;36mpredict\u001b[0;34m(self, X)\u001b[0m\n\u001b[1;32m    308\u001b[0m         \u001b[0my_pred\u001b[0m \u001b[0;34m:\u001b[0m \u001b[0marray\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mshape\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mn_samples\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    309\u001b[0m         \"\"\"\n\u001b[0;32m--> 310\u001b[0;31m         \u001b[0mX\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_validate_for_predict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    311\u001b[0m         \u001b[0mpredict\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_sparse_predict\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_sparse\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dense_predict\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    312\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/mh/anaconda3/lib/python3.6/site-packages/sklearn/svm/base.py\u001b[0m in \u001b[0;36m_validate_for_predict\u001b[0;34m(self, X)\u001b[0m\n\u001b[1;32m    477\u001b[0m             raise ValueError(\"X.shape[1] = %d should be equal to %d, \"\n\u001b[1;32m    478\u001b[0m                              \u001b[0;34m\"the number of features at training time\"\u001b[0m \u001b[0;34m%\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 479\u001b[0;31m                              (n_features, self.shape_fit_[1]))\n\u001b[0m\u001b[1;32m    480\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    481\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: X.shape[1] = 9161 should be equal to 781, the number of features at training time"
     ]
    }
   ],
   "source": [
    "stopset = set(nltk.corpus.stopwords.words('english'))\n",
    "stopset.add(\"\\'\")\n",
    "stopset.add(\"\\\"\")\n",
    "\n",
    "reddit_train = load_files(\"/media/mh/EF2A-B9DB/Reddit_Data/Train/\")\n",
    "X, y = reddit_train.data, reddit_train.target\n",
    "# Check the data\n",
    "\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.25, random_state=42)\n",
    "\n",
    "print(\"samples per class: {}\".format(np.bincount(y_train)))\n",
    "print(\"Data: {}\".format(np.bincount(y_test)))\n",
    "#print(X_train.shape)\n",
    "\n",
    "\n",
    "combs = make_combs()\n",
    "combs = [('j')]\n",
    "for comb in combs:\n",
    "    print(comb)\n",
    "    if len(comb) < 3:\n",
    "        vect = CountVectorizer(tokenizer=lambda doc: get_selected_lemma(doc,comb), stop_words =stopset, min_df=2)\n",
    "       \n",
    "        X_train = vect.fit_transform(X_train)\n",
    "        print('X_train.shape:\\n{}'.format(X_train.shape))\n",
    "\n",
    "        mnNB = MultinomialNB()\n",
    "\n",
    "\n",
    "        print(\"\\nno feature selection\\n\")\n",
    "\n",
    "        X_test = vect.transform(X_test) \n",
    "        #        print('X.shape:\\n{}'.format(X_train.shape))\n",
    "\n",
    "        predict = mnNB.fit(X_train,y_train).predict(X_test)\n",
    "\n",
    "\n",
    "        predscore = mnNB.fit(X_train,y_train).score(X_test,y_test)\n",
    "        print('NB Test {:.3f}'.format(predscore))\n",
    "\n",
    "        print(classification_report(y_test, predict, target_names=['Dep','ND']))\n",
    "        #  print(confusion_matrix(y_test, y_pred, labels=range(n_classes)))\n",
    "\n",
    "        predict = svm.SVC().fit(X_train,y_train).predict(X_test)\n",
    "        predscore = svm.SVC().fit(X_train,y_train).score(X_test,y_test)\n",
    "        print(\"SVM Test score: {:.3f}\".format(predscore))\n",
    "        print(classification_report(y_test, predict, target_names=['Dep','ND']))\n",
    "\n",
    "        predict = LogisticRegression().fit(X_train,y_train).predict(X_test)\n",
    "        predscore = LogisticRegression().fit(X_train,y_train).score(X_test,y_test)\n",
    "        print(\"LR Test score: {:.3f}\".format(predscore))\n",
    "        print(classification_report(y_test, predict, target_names=['Dep','ND']))\n",
    "\n",
    "        print('feature selection\\n')\n",
    "        lr = LogisticRegression(C=1, penalty='l1').fit(X_train, y_train)\n",
    "        model = SelectFromModel(lr, prefit=True)\n",
    "        X_new = model.transform(X_train)\n",
    "        print(X_new.shape)\n",
    "\n",
    "        #print(X_new.get_feature_names())\n",
    "        X_test_1= model.transform(X_test)\n",
    "        #print(\"X_test:\\n{}\".format(repr(X_test_1)))\n",
    "\n",
    "        predict = mnNB.fit(X_train,y_train).predict(X_test)\n",
    "        predscore = mnNB.fit(X_new,y_train).score(X_test_1,y_test)\n",
    "        print('NB Test {:.3f}'.format(predscore))\n",
    "        print(classification_report(y_test, predict, target_names=['Dep','ND']))\n",
    "\n",
    "        predscore = svm.SVC().fit(X_new,y_train).score(X_test_1,y_test)\n",
    "        print(\"SVM Test score: {:.3f}\".format(predscore))\n",
    "        predict = svm.SVC().fit(X_new,y_train).predict(X_test)\n",
    "        print(classification_report(y_test, predict, target_names=['Dep','ND']))\n",
    "\n",
    "        predscore = LogisticRegression().fit(X_new, y_train).score(X_test_1,y_test)\n",
    "        print(\"LR Test score: {:.3f}\".format(predscore))\n",
    "        predict = LogisticRegression().fit(X_new,y_train).predict(X_test)\n",
    "        print(classification_report(y_test, predict, target_names=['Dep','ND']))\n",
    "        \n",
    "        print('\\n')\n",
    "        \n",
    "    \n",
    "        pca = TruncatedSVD(n_components=1000, random_state=0).fit(X_train)\n",
    "        #logistic = LogisticRegression()\n",
    "       # pipe = Pipeline(steps=[('pca', pca), ('logistic', logistic)])\n",
    "        X_train_pca = pca.transform(X_train)\n",
    "        X_test_pca = pca.transform(X_test)\n",
    "        print(\"X_train_pca.shape: {}\".format(X_train_pca.shape))\n",
    "        \n",
    "        \n",
    "        \n",
    "        predscore = GaussianNB().fit(X_train_pca,y_train).score(X_test_pca,y_test)\n",
    "        print('GaussianNB Test {:.3f}'.format(predscore))\n",
    "        \n",
    "        print(classification_report(y_test, predict, target_names=['Dep','ND']))\n",
    "       # print(confusion_matrix(y_test, y_pred, labels=range(n_classes)))\n",
    "\n",
    "        predict = svm.SVC().fit(X_train_pca,y_train).predict(X_test_pca)\n",
    "        predscore = svm.SVC().fit(X_train_pca,y_train).score(X_test_pca,y_test)\n",
    "        print(\"SVM Test score: {:.3f}\".format(predscore))\n",
    "        print(classification_report(y_test, predict, target_names=['Dep','ND']))\n",
    "\n",
    "        predict = LogisticRegression().fit(X_train_pca,y_train).predict(X_test_pca)\n",
    "        predscore = LogisticRegression().fit(X_train_pca,y_train).score(X_test_pca,y_test)\n",
    "        print(\"LR Test score: {:.3f}\".format(predscore))\n",
    "        print(classification_report(y_test, predict, target_names=['Dep','ND']))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_train_pca.shape: (1599, 100)\n",
      "GaussianNB Test 0.540\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "        Dep       0.80      0.02      0.03       255\n",
      "         ND       0.52      1.00      0.69       278\n",
      "\n",
      "avg / total       0.66      0.53      0.37       533\n",
      "\n",
      "SVM Test score: 0.574\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "        Dep       0.64      0.25      0.36       255\n",
      "         ND       0.56      0.87      0.68       278\n",
      "\n",
      "avg / total       0.60      0.57      0.53       533\n",
      "\n",
      "LR Test score: 0.627\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "        Dep       0.62      0.55      0.59       255\n",
      "         ND       0.63      0.69      0.66       278\n",
      "\n",
      "avg / total       0.63      0.63      0.62       533\n",
      "\n"
     ]
    }
   ],
   "source": [
    "        pca = TruncatedSVD(n_components=100, random_state=0).fit(X_train)\n",
    "        #logistic = LogisticRegression()\n",
    "       # pipe = Pipeline(steps=[('pca', pca), ('logistic', logistic)])\n",
    "        X_train_pca = pca.transform(X_train)\n",
    "        X_test_pca = pca.transform(X_test)\n",
    "        print(\"X_train_pca.shape: {}\".format(X_train_pca.shape))\n",
    "        \n",
    "        \n",
    "        \n",
    "        predscore = GaussianNB().fit(X_train_pca,y_train).score(X_test_pca,y_test)\n",
    "        print('GaussianNB Test {:.3f}'.format(predscore))\n",
    "        \n",
    "        print(classification_report(y_test, predict, target_names=['Dep','ND']))\n",
    "       # print(confusion_matrix(y_test, y_pred, labels=range(n_classes)))\n",
    "\n",
    "        predict = svm.SVC().fit(X_train_pca,y_train).predict(X_test_pca)\n",
    "        predscore = svm.SVC().fit(X_train_pca,y_train).score(X_test_pca,y_test)\n",
    "        print(\"SVM Test score: {:.3f}\".format(predscore))\n",
    "        print(classification_report(y_test, predict, target_names=['Dep','ND']))\n",
    "\n",
    "        predict = LogisticRegression().fit(X_train_pca,y_train).predict(X_test_pca)\n",
    "        predscore = LogisticRegression().fit(X_train_pca,y_train).score(X_test_pca,y_test)\n",
    "        print(\"LR Test score: {:.3f}\".format(predscore))\n",
    "        print(classification_report(y_test, predict, target_names=['Dep','ND']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# char-ngrams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "samples per class: [759 840]\n",
      "Data: [255 278]\n",
      "6\n",
      "samples per class: [759 840]\n",
      "Data: [255 278]\n",
      "X_train.shape:\n",
      "(1599, 1806976)\n",
      "\n",
      "no feature selection\n",
      "\n",
      "NB Test 0.662\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "        Dep       0.77      0.42      0.54       255\n",
      "         ND       0.62      0.89      0.73       278\n",
      "\n",
      "avg / total       0.70      0.66      0.64       533\n",
      "\n",
      "SVM Test score: 0.687\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "        Dep       0.68      0.61      0.64       255\n",
      "         ND       0.67      0.74      0.70       278\n",
      "\n",
      "avg / total       0.68      0.68      0.67       533\n",
      "\n",
      "LR Test score: 0.687\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "        Dep       0.70      0.61      0.65       255\n",
      "         ND       0.68      0.76      0.72       278\n",
      "\n",
      "avg / total       0.69      0.69      0.68       533\n",
      "\n"
     ]
    },
    {
     "ename": "MemoryError",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mMemoryError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-53-7f253afb8309>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     82\u001b[0m         \"\"\" \n\u001b[1;32m     83\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 84\u001b[0;31m         \u001b[0mpca\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTruncatedSVD\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mn_components\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m100\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrandom_state\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     85\u001b[0m         \u001b[0;31m#logistic = LogisticRegression()\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     86\u001b[0m        \u001b[0;31m# pipe = Pipeline(steps=[('pca', pca), ('logistic', logistic)])\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/mh/anaconda3/lib/python3.6/site-packages/sklearn/decomposition/truncated_svd.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, X, y)\u001b[0m\n\u001b[1;32m    133\u001b[0m             \u001b[0mReturns\u001b[0m \u001b[0mthe\u001b[0m \u001b[0mtransformer\u001b[0m \u001b[0mobject\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    134\u001b[0m         \"\"\"\n\u001b[0;32m--> 135\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit_transform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    136\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    137\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/mh/anaconda3/lib/python3.6/site-packages/sklearn/decomposition/truncated_svd.py\u001b[0m in \u001b[0;36mfit_transform\u001b[0;34m(self, X, y)\u001b[0m\n\u001b[1;32m    171\u001b[0m             U, Sigma, VT = randomized_svd(X, self.n_components,\n\u001b[1;32m    172\u001b[0m                                           \u001b[0mn_iter\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mn_iter\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 173\u001b[0;31m                                           random_state=random_state)\n\u001b[0m\u001b[1;32m    174\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    175\u001b[0m             \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"unknown algorithm %r\"\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0malgorithm\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/mh/anaconda3/lib/python3.6/site-packages/sklearn/utils/extmath.py\u001b[0m in \u001b[0;36mrandomized_svd\u001b[0;34m(M, n_components, n_oversamples, n_iter, power_iteration_normalizer, transpose, flip_sign, random_state)\u001b[0m\n\u001b[1;32m    362\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    363\u001b[0m     Q = randomized_range_finder(M, n_random, n_iter,\n\u001b[0;32m--> 364\u001b[0;31m                                 power_iteration_normalizer, random_state)\n\u001b[0m\u001b[1;32m    365\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    366\u001b[0m     \u001b[0;31m# project M to the (k + p) dimensional space using the basis vectors\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/mh/anaconda3/lib/python3.6/site-packages/sklearn/utils/extmath.py\u001b[0m in \u001b[0;36mrandomized_range_finder\u001b[0;34m(A, size, n_iter, power_iteration_normalizer, random_state)\u001b[0m\n\u001b[1;32m    256\u001b[0m             \u001b[0mQ\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msafe_sparse_dot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mA\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mT\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mQ\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    257\u001b[0m         \u001b[0;32melif\u001b[0m \u001b[0mpower_iteration_normalizer\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'LU'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 258\u001b[0;31m             \u001b[0mQ\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlinalg\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlu\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msafe_sparse_dot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mA\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mQ\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpermute_l\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    259\u001b[0m             \u001b[0mQ\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlinalg\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlu\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msafe_sparse_dot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mA\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mT\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mQ\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpermute_l\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    260\u001b[0m         \u001b[0;32melif\u001b[0m \u001b[0mpower_iteration_normalizer\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'QR'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/mh/anaconda3/lib/python3.6/site-packages/sklearn/utils/extmath.py\u001b[0m in \u001b[0;36msafe_sparse_dot\u001b[0;34m(a, b, dense_output)\u001b[0m\n\u001b[1;32m    182\u001b[0m     \"\"\"\n\u001b[1;32m    183\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0missparse\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0missparse\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mb\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 184\u001b[0;31m         \u001b[0mret\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0ma\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mb\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    185\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mdense_output\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mhasattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mret\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"toarray\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    186\u001b[0m             \u001b[0mret\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mret\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtoarray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/mh/anaconda3/lib/python3.6/site-packages/scipy/sparse/base.py\u001b[0m in \u001b[0;36m__mul__\u001b[0;34m(self, other)\u001b[0m\n\u001b[1;32m    358\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_mul_vector\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mother\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mravel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mM\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    359\u001b[0m             \u001b[0;32melif\u001b[0m \u001b[0mother\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mndim\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m2\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mother\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mN\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 360\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_mul_multivector\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mother\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    361\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    362\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0misscalarlike\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mother\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/mh/anaconda3/lib/python3.6/site-packages/scipy/sparse/compressed.py\u001b[0m in \u001b[0;36m_mul_multivector\u001b[0;34m(self, other)\u001b[0m\n\u001b[1;32m    505\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    506\u001b[0m         result = np.zeros((M,n_vecs), dtype=upcast_char(self.dtype.char,\n\u001b[0;32m--> 507\u001b[0;31m                                                         other.dtype.char))\n\u001b[0m\u001b[1;32m    508\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    509\u001b[0m         \u001b[0;31m# csr_matvecs or csc_matvecs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mMemoryError\u001b[0m: "
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "reddit_train = load_files(\"/media/mh/EF2A-B9DB/Reddit_Data/Train/\")\n",
    "X, y = reddit_train.data, reddit_train.target\n",
    "# Check the data\n",
    "\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.25, random_state=42)\n",
    "\n",
    "print(\"samples per class: {}\".format(np.bincount(y_train)))\n",
    "print(\"Data: {}\".format(np.bincount(y_test)))\n",
    "#print(X_train.shape)\n",
    "\n",
    "\n",
    "\n",
    "for n in range(6,7):\n",
    "    print(n)\n",
    "    if n !=0:\n",
    "        reddit_train = load_files(\"/media/mh/EF2A-B9DB/Reddit_Data/Train/\")\n",
    "        X, y = reddit_train.data, reddit_train.target\n",
    "        # Check the data\n",
    "\n",
    "\n",
    "        X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.25, random_state=42)\n",
    "\n",
    "        print(\"samples per class: {}\".format(np.bincount(y_train)))\n",
    "        print(\"Data: {}\".format(np.bincount(y_test)))\n",
    "        #print(X_train.shape)\n",
    "\n",
    "\n",
    "        vect = CountVectorizer(analyzer='char', ngram_range = (n, n),encoding='utf-8')\n",
    "       \n",
    "        X_train = vect.fit_transform(X_train)\n",
    "        print('X_train.shape:\\n{}'.format(X_train.shape))\n",
    "\n",
    "        mnNB = MultinomialNB()\n",
    "\n",
    "\n",
    "        print(\"\\nno feature selection\\n\")\n",
    "\n",
    "        X_test = vect.transform(X_test) \n",
    "        #        print('X.shape:\\n{}'.format(X_train.shape))\n",
    "\n",
    "        predict = mnNB.fit(X_train,y_train).predict(X_test)\n",
    "\n",
    "\n",
    "        predscore = mnNB.fit(X_train,y_train).score(X_test,y_test)\n",
    "        print('NB Test {:.3f}'.format(predscore))\n",
    "\n",
    "        print(classification_report(y_test, predict, target_names=['Dep','ND']))\n",
    "        #  print(confusion_matrix(y_test, y_pred, labels=range(n_classes)))\n",
    "\n",
    "        predict = LinearSVC().fit(X_train,y_train).predict(X_test)\n",
    "        predscore = LinearSVC().fit(X_train,y_train).score(X_test,y_test)\n",
    "        print(\"SVM Test score: {:.3f}\".format(predscore))\n",
    "        print(classification_report(y_test, predict, target_names=['Dep','ND']))\n",
    "\n",
    "        predict = LogisticRegression().fit(X_train,y_train).predict(X_test)\n",
    "        predscore = LogisticRegression().fit(X_train,y_train).score(X_test,y_test)\n",
    "        print(\"LR Test score: {:.3f}\".format(predscore))\n",
    "        print(classification_report(y_test, predict, target_names=['Dep','ND']))\n",
    "        \"\"\"\n",
    "        print('feature selection\\n')\n",
    "        lr = LogisticRegression(C=1, penalty='l1').fit(X_train, y_train)\n",
    "        model = SelectFromModel(lr, prefit=True)\n",
    "        X_new = model.transform(X_train)\n",
    "        print(X_new.shape)\n",
    "\n",
    "        #print(X_new.get_feature_names())\n",
    "        X_test_1= model.transform(X_test)\n",
    "        #print(\"X_test:\\n{}\".format(repr(X_test_1)))\n",
    "\n",
    "        predscore = mnNB.fit(X_new,y_train).score(X_test_1,y_test)\n",
    "        print('NB Test {:.3f}'.format(predscore))\n",
    "\n",
    "        predscore = LinearSVC().fit(X_new,y_train).score(X_test_1,y_test)\n",
    "        print(\"SVM Test score: {:.3f}\".format(predscore))\n",
    "\n",
    "        predscore = LogisticRegression().fit(X_new, y_train).score(X_test_1,y_test)\n",
    "        print(\"LR Test score: {:.3f}\".format(predscore))\n",
    "        \n",
    "        \"\"\" \n",
    "    \n",
    "        pca = TruncatedSVD(n_components=100, random_state=0).fit(X_train)\n",
    "        #logistic = LogisticRegression()\n",
    "       # pipe = Pipeline(steps=[('pca', pca), ('logistic', logistic)])\n",
    "        X_train_pca = pca.transform(X_train)\n",
    "        X_test_pca = pca.transform(X_test)\n",
    "        print(\"X_train_pca.shape: {}\".format(X_train_pca.shape))\n",
    "        \n",
    "        \n",
    "        \n",
    "        predscore = GaussianNB().fit(X_train_pca,y_train).score(X_test_pca,y_test)\n",
    "        print('GaussianNB Test {:.3f}'.format(predscore))\n",
    "        \n",
    "        print(classification_report(y_test, predict, target_names=['Dep','ND']))\n",
    "       # print(confusion_matrix(y_test, y_pred, labels=range(n_classes)))\n",
    "\n",
    "        predict = LinearSVC().fit(X_train_pca,y_train).predict(X_test_pca)\n",
    "        predscore = LinearSVC().fit(X_train_pca,y_train).score(X_test_pca,y_test)\n",
    "        print(\"SVM Test score: {:.3f}\".format(predscore))\n",
    "        print(classification_report(y_test, predict, target_names=['Dep','ND']))\n",
    "\n",
    "        predict = LogisticRegression().fit(X_train_pca,y_train).predict(X_test_pca)\n",
    "        predscore = LogisticRegression().fit(X_train_pca,y_train).score(X_test_pca,y_test)\n",
    "        print(\"LR Test score: {:.3f}\".format(predscore))\n",
    "        print(classification_report(y_test, predict, target_names=['Dep','ND']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# word n-grams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data: [1012 1118]\n",
      "X_train_charngram.shape:\n",
      "(1597, 30126)\n",
      "samples per class: [759 838]\n",
      "Data: [253 280]\n",
      "X_train.shape:\n",
      "(1597, 30126)\n",
      "\n",
      "no feature selection\n",
      "\n",
      "NB Test 0.670\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "        Dep       0.72      0.50      0.59       253\n",
      "         ND       0.65      0.82      0.72       280\n",
      "\n",
      "avg / total       0.68      0.67      0.66       533\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#\n",
    "#Classification using scikit-learn\n",
    "#\n",
    "\n",
    "#load data  (change the file path)\n",
    "reddit_train = load_files(\"/media/mh/EF2A-B9DB/Reddit_Data/Train/\")\n",
    "X, y = reddit_train.data, reddit_train.target\n",
    "# Check the data\n",
    "print(\"Data: {}\".format(np.bincount(y)))\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.25, random_state=42)\n",
    "\n",
    "    \n",
    "#vect = CountVectorizer(tokenizer=lemmatize,  stop_words =stopset, ngram_range = (3, 3) )\n",
    "\n",
    "vect = CountVectorizer(tokenizer=lambda doc: make_ngrams(doc,1), min_df=2)\n",
    "X_train = vect.fit_transform(X_train)\n",
    "X_test = vect.transform(X_test)\n",
    "print('X_train_charngram.shape:\\n{}'.format(X_train.shape))\n",
    "\n",
    "\n",
    "# To check features (the first 100)\n",
    "#feature_names = np.array(vect.get_feature_names())\n",
    "#print(feature_names[1000:1500])\n",
    "\n",
    "print(\"samples per class: {}\".format(np.bincount(y_train)))\n",
    "print(\"Data: {}\".format(np.bincount(y_test)))\n",
    "#print(X_train.shape)\n",
    "print('X_train.shape:\\n{}'.format(X_train.shape))\n",
    "\n",
    "mnNB = MultinomialNB()\n",
    "\n",
    "\n",
    "print(\"\\nno feature selection\\n\")\n",
    "\n",
    " \n",
    "#        print('X.shape:\\n{}'.format(X_train.shape))\n",
    "\n",
    "predict = mnNB.fit(X_train,y_train).predict(X_test)\n",
    "\n",
    "\n",
    "predscore = mnNB.fit(X_train,y_train).score(X_test,y_test)\n",
    "print('NB Test {:.3f}'.format(predscore))\n",
    "\n",
    "print(classification_report(y_test, predict, target_names=['Dep','ND']))\n",
    "#  print(confusion_matrix(y_test, y_pred, labels=range(n_classes)))\n",
    "\n",
    "predict = LinearSVC().fit(X_train,y_train).predict(X_test)\n",
    "predscore = LinearSVC().fit(X_train,y_train).score(X_test,y_test)\n",
    "print(\"SVM Test score: {:.3f}\".format(predscore))\n",
    "print(classification_report(y_test, predict, target_names=['Dep','ND']))\n",
    "\n",
    "predict = LogisticRegression().fit(X_train,y_train).predict(X_test)\n",
    "predscore = LogisticRegression().fit(X_train,y_train).score(X_test,y_test)\n",
    "print(\"LR Test score: {:.3f}\".format(predscore))\n",
    "print(classification_report(y_test, predict, target_names=['Dep','ND']))\n",
    "\"\"\"\n",
    "print('feature selection\\n')\n",
    "lr = LogisticRegression(C=1, penalty='l1').fit(X_train, y_train)\n",
    "model = SelectFromModel(lr, prefit=True)\n",
    "X_new = model.transform(X_train)\n",
    "print(X_new.shape)\n",
    "\n",
    "#print(X_new.get_feature_names())\n",
    "X_test_1= model.transform(X_test)\n",
    "#print(\"X_test:\\n{}\".format(repr(X_test_1)))\n",
    "\n",
    "predscore = mnNB.fit(X_new,y_train).score(X_test_1,y_test)\n",
    "print('NB Test {:.3f}'.format(predscore))\n",
    "\n",
    "predscore = LinearSVC().fit(X_new,y_train).score(X_test_1,y_test)\n",
    "print(\"SVM Test score: {:.3f}\".format(predscore))\n",
    "\n",
    "predscore = LogisticRegression().fit(X_new, y_train).score(X_test_1,y_test)\n",
    "print(\"LR Test score: {:.3f}\".format(predscore))\n",
    "\n",
    "\"\"\" \n",
    "\n",
    "pca = TruncatedSVD(n_components=1000, random_state=0).fit(X_train)\n",
    "#logistic = LogisticRegression()\n",
    "# pipe = Pipeline(steps=[('pca', pca), ('logistic', logistic)])\n",
    "X_train_pca = pca.transform(X_train)\n",
    "X_test_pca = pca.transform(X_test)\n",
    "print(\"X_train_pca.shape: {}\".format(X_train_pca.shape))\n",
    "\n",
    "\n",
    "\n",
    "predscore = GaussianNB().fit(X_train_pca,y_train).score(X_test_pca,y_test)\n",
    "print('GaussianNB Test {:.3f}'.format(predscore))\n",
    "\n",
    "print(classification_report(y_test, predict, target_names=['Dep','ND']))\n",
    "# print(confusion_matrix(y_test, y_pred, labels=range(n_classes)))\n",
    "\n",
    "predict = LinearSVC().fit(X_train_pca,y_train).predict(X_test_pca)\n",
    "predscore = LinearSVC().fit(X_train_pca,y_train).score(X_test_pca,y_test)\n",
    "print(\"SVM Test score: {:.3f}\".format(predscore))\n",
    "print(classification_report(y_test, predict, target_names=['Dep','ND']))\n",
    "\n",
    "predict = LogisticRegression().fit(X_train_pca,y_train).predict(X_test_pca)\n",
    "predscore = LogisticRegression().fit(X_train_pca,y_train).score(X_test_pca,y_test)\n",
    "print(\"LR Test score: {:.3f}\".format(predscore))\n",
    "print(classification_report(y_test, predict, target_names=['Dep','ND']))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "X has 386272 features per sample; expecting 878545",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-57-b65d64108a31>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mpredict\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mLinearSVC\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0my_train\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_test\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mpredscore\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mLinearSVC\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0my_train\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mscore\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_test\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0my_test\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"SVM Test score: {:.3f}\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpredscore\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mclassification_report\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpredict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget_names\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'Dep'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'ND'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/mh/anaconda3/lib/python3.6/site-packages/sklearn/linear_model/base.py\u001b[0m in \u001b[0;36mpredict\u001b[0;34m(self, X)\u001b[0m\n\u001b[1;32m    334\u001b[0m             \u001b[0mPredicted\u001b[0m \u001b[0;32mclass\u001b[0m \u001b[0mlabel\u001b[0m \u001b[0mper\u001b[0m \u001b[0msample\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    335\u001b[0m         \"\"\"\n\u001b[0;32m--> 336\u001b[0;31m         \u001b[0mscores\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdecision_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    337\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mscores\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    338\u001b[0m             \u001b[0mindices\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mscores\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mastype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mint\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/mh/anaconda3/lib/python3.6/site-packages/sklearn/linear_model/base.py\u001b[0m in \u001b[0;36mdecision_function\u001b[0;34m(self, X)\u001b[0m\n\u001b[1;32m    315\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0mn_features\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    316\u001b[0m             raise ValueError(\"X has %d features per sample; expecting %d\"\n\u001b[0;32m--> 317\u001b[0;31m                              % (X.shape[1], n_features))\n\u001b[0m\u001b[1;32m    318\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    319\u001b[0m         scores = safe_sparse_dot(X, self.coef_.T,\n",
      "\u001b[0;31mValueError\u001b[0m: X has 386272 features per sample; expecting 878545"
     ]
    }
   ],
   "source": [
    "predict = LinearSVC().fit(X_train,y_train).predict(X_test)\n",
    "predscore = LinearSVC().fit(X_train,y_train).score(X_test,y_test)\n",
    "print(\"SVM Test score: {:.3f}\".format(predscore))\n",
    "print(classification_report(y_test, predict, target_names=['Dep','ND']))\n",
    "\n",
    "predict = LogisticRegression().fit(X_train,y_train).predict(X_test)\n",
    "predscore = LogisticRegression().fit(X_train,y_train).score(X_test,y_test)\n",
    "print(\"LR Test score: {:.3f}\".format(predscore))\n",
    "print(classification_report(y_test, predict, target_names=['Dep','ND']))\n",
    "\"\"\"\n",
    "print('feature selection\\n')\n",
    "lr = LogisticRegression(C=1, penalty='l1').fit(X_train, y_train)\n",
    "model = SelectFromModel(lr, prefit=True)\n",
    "X_new = model.transform(X_train)\n",
    "print(X_new.shape)\n",
    "\n",
    "#print(X_new.get_feature_names())\n",
    "X_test_1= model.transform(X_test)\n",
    "#print(\"X_test:\\n{}\".format(repr(X_test_1)))\n",
    "\n",
    "predscore = mnNB.fit(X_new,y_train).score(X_test_1,y_test)\n",
    "print('NB Test {:.3f}'.format(predscore))\n",
    "\n",
    "predscore = LinearSVC().fit(X_new,y_train).score(X_test_1,y_test)\n",
    "print(\"SVM Test score: {:.3f}\".format(predscore))\n",
    "\n",
    "predscore = LogisticRegression().fit(X_new, y_train).score(X_test_1,y_test)\n",
    "print(\"LR Test score: {:.3f}\".format(predscore))\n",
    "\n",
    "\"\"\" \n",
    "\n",
    "pca = TruncatedSVD(n_components=1000, random_state=0).fit(X_train)\n",
    "#logistic = LogisticRegression()\n",
    "# pipe = Pipeline(steps=[('pca', pca), ('logistic', logistic)])\n",
    "X_train_pca = pca.transform(X_train)\n",
    "X_test_pca = pca.transform(X_test)\n",
    "print(\"X_train_pca.shape: {}\".format(X_train_pca.shape))\n",
    "\n",
    "\n",
    "\n",
    "predscore = GaussianNB().fit(X_train_pca,y_train).score(X_test_pca,y_test)\n",
    "print('GaussianNB Test {:.3f}'.format(predscore))\n",
    "\n",
    "print(classification_report(y_test, predict, target_names=['Dep','ND']))\n",
    "# print(confusion_matrix(y_test, y_pred, labels=range(n_classes)))\n",
    "\n",
    "predict = LinearSVC().fit(X_train_pca,y_train).predict(X_test_pca)\n",
    "predscore = LinearSVC().fit(X_train_pca,y_train).score(X_test_pca,y_test)\n",
    "print(\"SVM Test score: {:.3f}\".format(predscore))\n",
    "print(classification_report(y_test, predict, target_names=['Dep','ND']))\n",
    "\n",
    "predict = LogisticRegression().fit(X_train_pca,y_train).predict(X_test_pca)\n",
    "predscore = LogisticRegression().fit(X_train_pca,y_train).score(X_test_pca,y_test)\n",
    "print(\"LR Test score: {:.3f}\".format(predscore))\n",
    "print(classification_report(y_test, predict, target_names=['Dep','ND']))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from nltk.parse.stanford import StanfordDependencyParser\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
